{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdaBoost.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAqHliqjGs9dzDrGq/4Wfj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raaraya1/Personal-Proyects/blob/main/Canales%20de%20Youtube/Python%20Engineer/AdaBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AdaBoost**\n",
        "\n",
        "Este algoritmo se basa en ir agrupando otros algoritmos de clasificacion, para que en conjunto generen una prediccion.\n",
        "\n",
        "Asimismo y, a diferencia del algoritmo de Random Forest, es que el **voto** de cada estimador no valen lo mismo, es decir, existe un grado de importancia (**weight**) entre los estimadores que siendo estos ponderados por sus votos es que generan la prediccion del algoritmo.\n",
        "\n",
        "**Weak Learner (Decision Stump)**\n",
        "\n",
        "Es un algoritmo que sencillamente clasifica los datos segun un limite (similar a uno de los pasos del algoritmo de Decision Tree)\n",
        "\n",
        "**Error**\n",
        "\n",
        "- Primera itereacion\n",
        "\n",
        "$$\n",
        "ϵ_{1} = \\frac{desaceirtos}{N}\n",
        "$$\n",
        "\n",
        "- A partir de la segunda iteracion\n",
        "\n",
        "$$\n",
        "ϵ_{t} = \\sum weights\n",
        "$$\n",
        "\n",
        "Nota: Si el error es mayor a 0.5, se itercambia la clasificacion y se calcula el $error = 1 - error$\n",
        "\n",
        "**Weights**\n",
        "\n",
        "- Al inicio\n",
        "$$\n",
        "w_{0} = \\frac{1}{N} para cada muestra\n",
        "$$\n",
        "\n",
        "- Luego\n",
        "\n",
        "$$\n",
        "w = \\frac{w \\cdot e^{- αyh(X)}}{\\sum w} \n",
        "$$\n",
        "\n",
        "**Performance**\n",
        "\n",
        "$$\n",
        "\\alpha = 0.5 \\cdot log(\\frac{1-ϵ_{t}}{ϵ_{t}})\n",
        "$$\n",
        "\n",
        "**Prediction**\n",
        "\n",
        "$$\n",
        "y = sign(\\sum_{t}^{T} α_{t} \\cdot h(X))\n",
        "$$\n",
        "\n",
        "**Training**\n",
        "\n",
        "Se inicializan los pesos de cada mustra en $\\frac{1}{N}$\n",
        "\n",
        "- Entrenamos a un clasificador debil (se busca la mejor variable y limite para segmentar)\n",
        "- Calculamos el error $ϵ_{t} = \\sum_{desaciertos} weights$\n",
        " - Cambiar el error y la polaridad si este es mayor a 0.5\n",
        "- Calcular $ \\alpha = 0.5 \\cdot log(\\frac{1 - \\epsilon_{t}}{ϵ_{t}})$\n",
        "- Actualizar los pesos: $w = \\frac{w \\cdot e^{- αh(X)}}{Z}$\n"
      ],
      "metadata": {
        "id": "SeI2ZSLkwmI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Armando el algoritmo desde cero**"
      ],
      "metadata": {
        "id": "2zeJbhDgVhDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5tARIKzTwcOv"
      },
      "outputs": [],
      "source": [
        "# Importamos la biblioteca\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clasificador debil (DecisionStump)\n",
        "class DecisionStump:\n",
        "  def __init__(self):\n",
        "    self.polarity = 1\n",
        "    self.feature_idx = None\n",
        "    self.threshold = None \n",
        "    self.alpha = None \n",
        "\n",
        "  def predict(self, X):\n",
        "    n_samples = X.shape[0]\n",
        "    X_column = X[:, self.feature_idx]\n",
        "    predictions = np.ones(n_samples)\n",
        "    if self.polarity == 1:\n",
        "      predictions[X_column < self.threshold] = -1\n",
        "    else:\n",
        "      predictions[X_column > self.threshold] = -1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "class Adaboost:\n",
        "  def __init__(self, n_clf=5):\n",
        "    self.n_clf = n_clf\n",
        "    self.clfs = []\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # Inicializamos los pesos (1/N)\n",
        "    w = np.full(n_samples, (1/n_samples))\n",
        "\n",
        "    self.clfs = []\n",
        "\n",
        "    # Iteramos de clasificador en clasificador\n",
        "    for _ in range(self.n_clf):\n",
        "      clf = DecisionStump()\n",
        "      min_error = float(\"inf\")\n",
        "\n",
        "      # Busqueda del limite y la variable\n",
        "      for feature_i in range(n_features):\n",
        "        X_column = X[:, feature_i]\n",
        "        thresholds = np.unique(X_column) \n",
        "\n",
        "        for threshold in thresholds:\n",
        "          # Predecimos con polaridad 1\n",
        "          p = 1\n",
        "          predictions = np.ones(n_samples)\n",
        "          predictions[X_column < threshold] = -1\n",
        "\n",
        "          # Error = suma de los pesos de los desaciertos\n",
        "          misclassified = w[y != predictions]\n",
        "          error = sum(misclassified)\n",
        "\n",
        "          if error > 0.5:\n",
        "            error = 1 - error \n",
        "            p = -1\n",
        "\n",
        "          # Guardar la mejor configuracion\n",
        "          if error < min_error:\n",
        "            clf.polarity = p \n",
        "            clf.threshold = threshold\n",
        "            clf.feature_idx = feature_i\n",
        "            min_error = error\n",
        "\n",
        "    # Calcular el alpha\n",
        "    EPS = 1e-10 \n",
        "    clf.alpha = 0.5*np.log((1.0 - min_error + EPS)/(min_error + EPS))\n",
        "\n",
        "    # Calcular las predicciones y actualizar los pesos \n",
        "    predictions = clf.predict(X)\n",
        "\n",
        "    w *= np.exp(-clf.alpha * y * predictions)\n",
        "\n",
        "    # Noramlizamos a 1\n",
        "    w /= np.sum(w)\n",
        "\n",
        "    # Guardamos el clasificador\n",
        "    self.clfs.append(clf)\n",
        "\n",
        "  def predict(self, X):\n",
        "    clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
        "    y_pred = np.sum(clf_preds, axis=0)\n",
        "    y_pred = np.sign(y_pred)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "ZQB2R85iVsWe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ahora probemos el algorimto**"
      ],
      "metadata": {
        "id": "pHoXtfFOaBwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las bibliotecas\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "A0rUoh9IaBCs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos una funcion para medir el rendimiento\n",
        "def accuracy(y_true, y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "  return accuracy\n",
        "\n",
        "# Cargamos los datos\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Adaptamos el un poco el problema para el algoritmo\n",
        "y[y == 0] = -1\n",
        "\n",
        "# Separamos en set de entrenamiento y validacion\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
        "\n",
        "# Adaboost classification con 5 clasificadores debiles\n",
        "clf = Adaboost(n_clf=5)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "acc = accuracy(y_test, y_pred)\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gf4vzaOaQRO",
        "outputId": "fc0fb34b-499f-4284-b247-ba126ac9a5b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9122807017543859"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ahora probemos desde sklearn**\n"
      ],
      "metadata": {
        "id": "RaHX594BblzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos el clasificador\n",
        "from sklearn.ensemble import AdaBoostClassifier as ABC"
      ],
      "metadata": {
        "id": "V4cHLIT9bq9m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el clasificador, lo entrenamos y lo evaluamos\n",
        "clf_sk = ABC(n_estimators=5, random_state=5)\n",
        "clf_sk.fit(X_train, y_train)\n",
        "y_pred_sk = clf_sk.predict(X_test)\n",
        "\n",
        "acc_sk = accuracy(y_test, y_pred_sk)\n",
        "acc_sk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuQ3t0Tqb-OD",
        "outputId": "016219f7-4751-4544-e655-1de2e6308b1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9649122807017544"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}